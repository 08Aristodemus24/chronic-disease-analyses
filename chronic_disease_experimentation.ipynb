{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from utilities.preprocessors import column_summary \n",
    "from utilities.visualizers import disp_cat_feat, view_feat_outliers\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read dataset\n",
    "find the dataset here: https://www.kaggle.com/datasets/irakozekelly/u-s-chronic-disease-indicators-2023-release?resource=download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/chronic-disease-data/U.S._Chronic_Disease_Indicators__CDI___2023_Release.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_summary(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = df.select_dtypes(include=[float, int]).columns\n",
    "cat_cols = df.select_dtypes(include=[object, \"datetime\"]).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### first half of categorical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols[:cat_cols.shape[0] // 2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disp_cat_feat(df, cat_cols[:cat_cols.shape[0] // 2], fig_size=(50, 50), fig_dims=(7, 2), img_title=\"chronic disease 1st half of categorical features value counts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### second half of categorical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols[cat_cols.shape[0] // 2:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disp_cat_feat(df, cat_cols[cat_cols.shape[0] // 2:], fig_size=(50, 50), fig_dims=(8, 2), img_title=\"chronic disease 2nd half of categorical features value counts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Useful columns seem to be `LocationAbbr`, `LocationDesc`, `Question`, `DataSource`, `Topic`, `Stratification1`, `StratificationCategory`, Geolocation\n",
    "\n",
    "#### Useless columns seem to be `Response`, `DateValueFootnoteSymbol`, `DataValueFootnote`, `Stratification2`, `StratificationCategory2`, `Stratification3`, `StratificationCategory3`, `ResponseID`, `StratificationCategory1ID`, `StratificationID1`, `StratificationCategory2ID`, `StratificationID2`, `StratificationCategory3ID`, `StratificationID3`\n",
    "\n",
    "#### Columns that could be removed but should be screened further seem to be `DataValueUnit`, `DataValueTypeID`, `DataValueType`, `DataValue`, `TopicID`, `QuestionID`, `LocationID`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=[\"Response\",\n",
    "    \"ResponseID\",\n",
    "    \"DataValueFootnoteSymbol\",\n",
    "    \"DatavalueFootnote\",\n",
    "\n",
    "    \"StratificationCategory2\",\n",
    "    \"Stratification2\",\n",
    "    \"StratificationCategory3\",\n",
    "    \"Stratification3\",\n",
    "\n",
    "    \"StratificationCategoryID1\",\n",
    "    \"StratificationID1\",\n",
    "    \"StratificationCategoryID2\",\n",
    "    \"StratificationID2\",\n",
    "    \"StratificationCategoryID3\",\n",
    "    \"StratificationID3\"])\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Geolocation` is still a tuple, why not separate it instead into latitude and longitude values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_loc = df.iloc[0][\"GeoLocation\"]\n",
    "geo_loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = re.sub(r\"(POINT|[/(/)])\", \"\", geo_loc)\n",
    "test = test.strip()\n",
    "test = test.split(\" \")\n",
    "latitude, longitude = ast.literal_eval(test[0]), ast.literal_eval(test[1]) \n",
    "latitude, longitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lat_long(geo_loc):\n",
    "    \n",
    "    if pd.isna(geo_loc):\n",
    "        return (np.nan, np.nan)\n",
    "    \n",
    "    # if geoloc is not null or nan extract its\n",
    "    # longitude and latitude \n",
    "    # print(geo_loc)\n",
    "    test = re.sub(r\"(POINT|[/(/)])\", \"\", geo_loc)\n",
    "    test = test.strip()\n",
    "    test = test.split(\" \")\n",
    "    latitude, longitude = ast.literal_eval(test[0]), ast.literal_eval(test[1]) \n",
    "    \n",
    "    return latitude, longitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lat_longs = df[\"GeoLocation\"].apply(get_lat_long).to_list()\n",
    "lat_longs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Latitude\"], df[\"Longitude\"] = list(zip(*lat_longs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can already drop `GeoLocation` as we havee already extracted the `latitude` and `longitude` values from its tuple that way we don't get any sql rollback errors anymore as we won't have to push this tuple into a database which is a datatype that does not exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=[\"GeoLocation\"], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can also drop the `topicID`, `questionID`, and `locationID` columsn as these are now irrelevant to our analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=[\"TopicID\", \"QuestionID\", \"LocationID\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Oh ok so the reason why the `topic`, `question`, `datavalueunit` and `datavalue`/`datavaluealt` columns are important is because for example the `topic` is `alchohol`, `question` is `alcohol use amoung youth?` (or more accurately what is the \"count\" of alcohol among youth), `datavalueunit` is `%`, and `datavalue` is `36.7` entails that *in the year 2013 in connecticut the alcohol use among youth was 36.7%*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"DataValueUnit\"].value_counts().index.to_list()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"DataValue\"].value_counts().index.to_list()[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"DataValueAlt\"].value_counts().index.to_list()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sometimes however these key columns like `datavalueunit` and `datavalue`/`datavaluealt` maybe null and so sometimes we may not need to include rows with columns that have these null values. Moreover `datavalue` has some values which are not null but instead are empty strings; these should be considered null and so cleaning the dataframe to fill in these empty strings with null values instead should be done and then convert this column to a float instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def str_to_float(data_value):\n",
    "    \"\"\"\n",
    "    converts string values of a column with \"\", \"no\"\n",
    "    into nan values and string floats to real float\n",
    "    values e.g. '1.4' to 1.4\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return float(data_value)\n",
    "\n",
    "    except:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"DataValue\"] = df[\"DataValue\"].apply(str_to_float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"DataValue\"].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"DataValueAlt\"].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[~df[\"DataValue\"].isna() | ~df[\"DataValueAlt\"].isna()].sample(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Because we already filled in the empty strings with null values instead and converted the supposed string column `datavalue` to a float instead we can drop rows with `datavalue` and `datavaluealt` columns that have null values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df[\"DataValueUnit\"] == \"cases per 1,000,000\"].sample(n=5, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Because `datavalueunit` also has null values we need to drop rows with these null values apart from the `datavalue` and `datavaluealt` columns. So if either the `datavalueunit` is null or `datavalue`/`datavaluealt` is null or both then we need to drop these rows. As for example `cases per 1,000,000` isn't really useful if `datavalue` is null, buti sana kung `cases per 1,000,000` and `datavalue` is `34.1` then this can be interpreted as *9 cases per 1,000,000.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[~df[\"DataValueUnit\"].isna() & (~df[\"DataValue\"].isna() & ~df[\"DataValueAlt\"].isna())]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"DataValueUnit\"].value_counts().index.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df[\"DataValueUnit\"] == \"per 100,000\", \"DataValueUnit\"] = \"cases per 100,000\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_data_value_units = df[\"DataValueUnit\"].value_counts().index.to_list()\n",
    "unique_data_value_units"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here we can see that there are some redundant `datavalueunits` that we need to also format to be the same to each other. For instance cases per 100,000, per 100,000, (not `per 100,000 residents` since this entails close proximity unlike `per 100,000` or `cases per 100,000` which may imply differing proximities of populations). We can just update the table to have `cases per 100,000` if there are rows with `per 100,000` as its `datavalueunit` as we've done above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df[\"Question\"] == \"Chronic liver disease mortality\"].sample(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Another example that can be translated into insight are these rows here e.g. in 2012 in florida there were 7.7 cases per 100,000 that died from chronic disease mortality. Or 0.077% or  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_data_value_types = df[\"DataValueType\"].value_counts().index.tolist()\n",
    "unique_data_value_types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### It seems that columns like question, datavalueunit, datavaluetype, datavalue seem to be the most important. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(by=[\"Question\", \"DataValueUnit\", \"DataValueType\", \"DataValue\"]) \\\n",
    ".agg(new_col=(\"DataValue\", \"count\")) \\\n",
    ".sort_values(by=\"new_col\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(by=[\"Question\", \"DataValueUnit\", \"DataValueType\", \"DataValue\"])\\\n",
    ".agg(AggregationCount=(\"DataValue\", \"count\"), DataValueMean=(\"DataValue\", \"mean\"), DataValueSum=(\"DataValue\", \"sum\"))\\\n",
    ".sort_values(by=\"AggregationCount\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stratification category will also be useful in determining the total cases. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Stratification1\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"StratificationCategory1\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = df.sample(n=5)\n",
    "samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples.reset_index(drop=True).loc[2, \"Question\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "question: Mammography use among women aged 50-74 years\n",
    "data value unit: %\n",
    "data value type: age-adjusted prevalence\n",
    "data valueu 13.4\n",
    "year start: 2018\n",
    "year end: 2018\n",
    "location desc: michigan\n",
    "stratification: white, non hispanic\n",
    "\n",
    "we also see here that it's not only age brackets in a question like aged >= 64 years or aged < 5 yearss but we also need to look out for ranges like 50-74 years etc.\n",
    "\n",
    "so to make this into a tangible number these information would entail mammography use among women aged 50-74 years "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### When the aforementioned columns are grouped we are given multiple categories of insights chronic disease occurences. But the problem is di ko pa alam pano interpret yung `age-adjusted mean`, `age-adjusted rate`, `crude mean`, `cases per 100,000`, and how these correlate with each other and how to interpret these as numerical values instead?? Gusto ko sana malaman agad yung cases per state in a country depending on their category like `activity limitation due to arthritis among adults aged >= 18 years` like how many per state yung mga tao na may ganto??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### note you don't need to be perfect and see all values, di mo kilangan kumuha pa ng extra data para pa cover yung CDI ng 2015 to present since 2001 to 2014 lang ang meron dito."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### But it makes sense to convert `cases per 100,000`, `age-adujusted mean`, `0.90` to a meaningful absolute number to determine the cases of the chronic disease indicator and to do this we have to know the population of a state at a specific year in order to determine the accurate number of cases for the chronic disease indicator\n",
    "\n",
    "and so it is also important to collect other data containing the US population each per state from 2001 to 2023\n",
    "* https://fredaccount.stlouisfed.org/login/secure/ ~ scraped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"LocationDesc\"].value_counts().index.tolist()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "populations_per_state_01_21_df = pd.read_csv('./data/population-data/us_populations_per_state_2001_to_2021.csv', index_col=0)\n",
    "populations_per_state_01_21_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_summary(populations_per_state_01_21_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### let's clean this populations df first to remove the commas and decimal signs in the numbers so as to be converted to an int instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean population column\n",
    "populations_per_state_01_21_df[\"Population\"] = populations_per_state_01_21_df[\"Population\"].apply(lambda population: int(re.sub(r\"[.,]\", \"\", population)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df[~pd.isna(df[\"LowConfidenceLimit\"]) | ~pd.isna(df[\"HighConfidenceLimit\"])]\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df[~pd.isna(df[\"Latitude\"]) | ~pd.isna(df[\"Longitude\"])]\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_summary(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### we can omit `latitude`, `longitude`, `lowconfidencelimit`, and `highconfidencelimit` columns that have null values as these relative to the `datavalue`, `question`, `topic`, and `datvalueunit` columns may not be as important. So we add id column to dataframe in preparation for pushing this dataframe to the local database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.index + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"ID\"] = df.index + 1\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Further analysis to see what combinations can we make with the unique values of the `datavaluetype` and `datavalueunit` columns in order to calculate the tangible number of chronic disease cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_data_value_unit_types = df.groupby(by=[\"DataValueUnit\", \"DataValueType\"])\\\n",
    ".agg(Col1=(\"DataValue\", \"count\"))\\\n",
    ".sort_values(by=\"Col1\")\n",
    "grouped_data_value_unit_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_data_value_unit_types = sorted(grouped_data_value_unit_types.index.tolist(), key=lambda value: value[0])\n",
    "unique_data_value_unit_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell with different datavaluetypes and datavalueunits to get examples\n",
    "sample = df[(df[\"DataValueType\"] == \"US Dollars\") & (df[\"DataValueUnit\"] == \"$\")].sample()[[\"DataValueUnit\", \"Question\", \"DataValue\"]].iloc[-1]\n",
    "sample[\"Question\"], sample[\"DataValueUnit\"], sample[\"DataValue\"],"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### we would have the following calculations for each unique pair of `datavalueunit` and `datavaluetype`\n",
    "the most important ones that maybe indicate chronic disease would be the ff. :\n",
    "* %, Prevalence e.g. `prevalence of gastrointestinal diabetes is 3.6%`\n",
    "question: `so if I have a question \"prevalence of gastrointestinal diabetes\", datavaluetype of prevalence, datavalueunit of %, and datavalue of 3.6 which when translated would be \"prevalence of gastrointestinal diabetes is 3.6%\". How can I then convert this to a tangible number given the population of let's say the state of arizona in 2015 which is 18003249`\n",
    "answer: $(data value / 100) * population = estimated cases$ \n",
    "\n",
    "\n",
    "\n",
    "* %, Percent e.g. `Live births occurring at Baby Friendly Facilities is 3.6%`\n",
    "question: `so if I have a question \"live births occuring at baby facilities\", datavaluetype of percent, datavalueunit of %, and datavalue of 3.6 which when translated would be \"live births occuring at baby facilities is 3.6%\". How can I then convert this to a tangible number given the population of let's say the state of arizona in 2015 which is 18003249`\n",
    "answer: To convert this percentage into a tangible number, you need: Total live births in Arizona (2015), not the total population.\n",
    "\n",
    "Example: If Arizona had 85,000 live births in 2015, then: Births at facilities = 0.036 × 85,000 = 3,060 births. Births at facilities=0.036×85,000=3,060 births. Why not use total population? The metric is a % of births, not the general population. Most of the 18M people in Arizona (e.g., elderly, children) didn’t give birth\n",
    "\n",
    "\n",
    "\n",
    "* %, Age-adjusted Prevalence e.g. `Current smokeless tobacco use among adults aged >= 18 years is 3.6%` (ah okay so gets ko na pag age adjusted may age number sa question)\n",
    "question: `so if I have a question \"Current smokeless tobacco use among adults aged >= 18 years\", datavaluetype of age-adjusted prevalence, datavalueunit of %, and datavalue of 3.6 which when translated would be \"Current smokeless tobacco use among adults aged >= 18 years is 3.6%\". How can I then convert this to a tangible number given the population of let's say the state of arizona in 2015 which is 18003249`\n",
    "answer: Step 2: Isolate the Adult Population (18+)\n",
    "Since the metric applies only to adults (≥18 years), you need:\n",
    "\n",
    "Total adult population (not total state population). Example: If adults make up ~75% of Arizona's population (2015):\n",
    "\n",
    "Adult population = 0.75 × 18,003,249=13,502,437 adults. Adult population=0.75×18,003,249=13,502,437 adults.\n",
    "\n",
    "\n",
    "\n",
    "* %, Crude Prevalence e.g. `Individuals meeting aerobic physical activity guidelines for substantial health benefits among adults aged >= 18 years is 3.6%`\n",
    "question: `so if I have a question \"prevalence of gastrointestinal diabetes\", datavaluetype of prevalence, datavalueunit of %, and datavalue of 3.6 which when translated would be \"prevalence of gastrointestinal diabetes is 3.6%\". How can I then convert this to a tangible number given the population of let's say the state of arizona in 2015 which is 18003249`\n",
    "\n",
    "* Number, Median e.g. `Median daily frequency of fruit consumption among adults aged >= 18 years is 3.6`\n",
    "* Number, Age-adjusted Mean e.g. `Average binge drinking frequency among adults aged >= 18 yeras who binge drink is 3.6`\n",
    "* Number, Mean e.g. `Average recent physically unhealthy days among adults aged >= 18 years is 3.6`\n",
    "* Number, Number e.g. `Mortality from cerebrovascular disease (stroke) is 2735`\n",
    "\n",
    "* cases per 1,000, Age-adjusted Rate e.g. `age-adjusted rate of chronic liver disease mortality is 14.4 cases per 1000`\n",
    "* cases per 10,000, Age-adjusted Rate e.g. `age-adjusted rate of hospitalization for chronic obstructive pulmonary disease as any diagnosis is 178.99 cases per 10000`\n",
    "* cases per 100,000, Age-adjusted Rate e.g. `age-adjusted rate of mortality from total cardiovascular diseases is 206.5 cases per 100000`\n",
    "* cases per 1,000,000, Age-adjusted Rate e.g. `age-adjusted rate of asthma mortality rate is 8.3 cases per 1000000`\n",
    "\n",
    "* cases per 1,000, Crude Rate e.g. `crude rate of hospitalization for heart failure among medicare-eligible persons aged >= 65 years is 16.5 cases per 1000`\n",
    "* cases per 10,000, Crude Rate e.g. `crude rate of Emergency department visit rate for asthma is 132.33 cases per 10000`\n",
    "* cases per 100,000, Crude Rate e.g. `crude rate of mortality from total cardiovascular diseases is 53 cases per 100000`\n",
    "* cases per 1,000,000, Crude Rate e.g. `crude rate of asthma mortality rate is 9.3 cases per 1000000`\n",
    "\n",
    "* cases per 1,000,000, Adjusted by age, sex, race and ethnicity e.g. `cases per 1000000 of Incidence of treated end-stage renal disease attributed to diabetes adjusted by age, sex, race and ethnicity is 156.8`\n",
    "* cases per 1,000,000, Number e.g. `cases per 1000000 of Incidence of treated end-stage renal disease is 1125`\n",
    "\n",
    "*NOTE: annual is yearly*\n",
    "* cases per 100,000, Average Annual Age-adjusted Rate e.g. `Average yearly age-adjusted rate of Invasive cancer (all sites combined), mortality is 11.9 cases per 100000`\n",
    "* cases per 100,000, Average Annual Crude Rate e.g. `Average yearly Crude Rate of Invasive cancer of the female breast, incidence is 13.3 cases er 100000`\n",
    "\n",
    "* cases per 100,000, Number e.g. `mortality from heart failure is 377599 cases per 100000`\n",
    "* per 100,000 residents, Number e.g. `Number of farmers markets per 100,000 residents is 7.3`\n",
    "\n",
    "and other miscellaneous pairs would be:\n",
    "* gallons, Per capita alcohol consumption e.g. per capita alchol consumption aged >= 14 years is 2.1 gallons\n",
    "* pack sales per capita, Number e.g. sale of cigarette packs is 44.3 pack sales per capita\n",
    "* Years, Number e.g. Life expectancy at birth is 78.2 years\n",
    "* `$`, US Dollars e.g. Amount of alcohol excise tax by beverage type (wine) is 0.72$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match = re.search(r\"aged [><]*[=]* \\d+ years\", \"crude rate of hospitalization for heart failure among medicare-eligible persons aged > 65 years\")\n",
    "match[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### It is imperative to also know the age brackets when the datavalueunit has an age adjusted unit because using the total population to calculate a tangible number out of these datavalueunits, datavaluetypes, and datavalues would not be useful for questions which measured the rates or numbers to be in a certain age bracket, and so we need to determine the population of this age bracket and only then can we calculate the estimated cases of the chronic disease for this age bracket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_brackets = df[\"Question\"].apply(lambda question: None if not re.search(r\"aged [><]*[=]* \\d+ years\", question) else re.search(r\"aged [><]*[=]* \\d+ years\", question)[0]).value_counts().index.tolist()\n",
    "age_brackets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Because having the age adjusted populations is important to calculate the number cases in age adjusted rate values we need to somehow collate extra data that has population values per age bracket or per age. Below is all the dta scraped from \n",
    "https://www.census.gov/data/tables/time-series/demo/popest/intercensal-2000-2010-state.html\n",
    "https://www.census.gov/data/tables/time-series/demo/popest/2010s-state-detail.html\n",
    "https://www.census.gov/data/tables/time-series/demo/popest/2020s-state-detail.html\n",
    "by running the notebook `extract_population_with_age_per_us_state.ipynb` followed by running the `population_experimentation.ipynb` notebook in order to model what was once incoherent excel spreadsheets into a tabular format with the age bracket, sex, state of a population "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![population data modelling 2000 to 2009 (3).png](./figures%20&%20images/population%20data%20modelling%202000%20to%202009%20(3).png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "populations_per_state_age_00_09_df = pd.read_csv('./data/population-data/us_populations_per_state_by_sex_and_age_2000_to_2009.csv', index_col=0)\n",
    "populations_per_state_age_00_09_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![population data modelling 2010 to 2019.png](./figures%20&%20images/population%20data%20modelling%202010%20to%202019.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "populations_per_state_age_10_19_df = pd.read_csv('./data/population-data/us_populations_per_state_by_sex_and_age_2010_to_2019.csv', index_col=0)\n",
    "populations_per_state_age_10_19_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "populations_per_state_age_20_23_df = pd.read_csv('./data/population-data/us_populations_per_state_by_sex_and_age_2020_to_2023.csv', index_col=0)\n",
    "populations_per_state_age_20_23_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# With the new dataframe we can import this to pgadmin4 using sqlalchemy and psycopg2 or to mysql server using sqlalchemy also and mysql_connector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import psycopg2 # for pgsql\n",
    "import mysql.connector as mysql_conn # for mysql\n",
    "import pyodbc\n",
    "import pandas as pd \n",
    "import os\n",
    "from pathlib import Path\n",
    "from sqlalchemy import create_engine \n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_dir = Path('./').resolve()\n",
    "load_dotenv(os.path.join(env_dir, '.env'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ff. is for postgresql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# credentials = {\n",
    "#     \"user\": os.environ[\"MY_SQL_USER\"],\n",
    "#     \"password\": os.environ[\"MY_SQL_PASSWORD\"],\n",
    "#     \"host\": os.environ[\"MY_SQL_HOST\"],\n",
    "#     \"database\": os.environ[\"MY_SQL_DATABASE\"],\n",
    "#     \"port\": os.environ[\"MY_SQL_PORT\"]\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # for postgresql only\n",
    "# conn_str = f'postgresql://{user}:{password}@{host}/{database}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# db = create_engine(conn_str)\n",
    "# conn = db.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conn = psycopg2.connect(conn_str)\n",
    "# conn.autocommit = True\n",
    "# cursor = conn.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### for mysql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# credentials = {\n",
    "#     \"user\": os.environ[\"MY_SQL_USER\"],\n",
    "#     \"password\": os.environ[\"MY_SQL_PASSWORD\"],\n",
    "#     \"host\": os.environ[\"MY_SQL_HOST\"],\n",
    "#     \"database\": os.environ[\"MY_SQL_DATABASE\"],\n",
    "#     \"port\": os.environ[\"MY_SQL_PORT\"]\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # for mysql only\n",
    "# conn_str = 'mysql+mysqlconnector://{user}:{password}@{host}:{port}/{database}'.format(**credentials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# db = create_engine(conn_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_sql(\n",
    "#     # name of sql table to be created in database\n",
    "#     name='chronicdisease', \n",
    "\n",
    "#     # \n",
    "#     con=db.connect(), \n",
    "#     if_exists='replace', \n",
    "\n",
    "#     # Write DataFrame index as a column. Uses 'index_label' as the column name in the table.\n",
    "#     # but since we already created an ID column there is no need\n",
    "#     index=False,\n",
    "\n",
    "#     # if a pendingrollbackerror is raised it is most likely due to \n",
    "#     # chunk_size parameter in to_sql(). The table may have of columns\n",
    "#     # or rows, so the size of the packets being sent may be above the\n",
    "#     # set threshold for the mysql database if no chunksize is set\n",
    "#     chunksize=1000\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### When we make queries in our mysql database via mysql server we get the ff."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "mysql> select id, yearstart, locationdesc, topic, datavalue, latitude, longitude from chronicdisease limit 3;\n",
    "+------+-----------+----------------------+---------+-----------+--------------------+--------------------+\n",
    "| id   | yearstart | locationdesc         | topic   | datavalue | latitude           | longitude          |\n",
    "+------+-----------+----------------------+---------+-----------+--------------------+--------------------+\n",
    "|    1 |      2013 | Connecticut          | Alcohol |      36.7 |  41.56266102000046 | -72.64984095199964 |\n",
    "|    2 |      2013 | District of Columbia | Alcohol |      31.4 |          38.907192 |         -77.036871 |\n",
    "|    3 |      2013 | Delaware             | Alcohol |      36.3 | 39.008830667000495 | -75.57774116799965 |\n",
    "+------+-----------+----------------------+---------+-----------+--------------------+--------------------+\n",
    "3 rows in set (0.00 sec)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### all we have to do now is be able to use this database in our notebook via our created connection using sqlalchemy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conn = mysql_conn.connect(**credentials)\n",
    "# conn.autocommit = True\n",
    "# cursor = conn.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### for microsoft sql server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credentials = {\n",
    "    \"user\": os.environ[\"MS_SQL_SERVER_USER\"],\n",
    "    \"password\": os.environ[\"MS_SQL_SERVER_PASSWORD\"],\n",
    "    \"host\": os.environ[\"MS_SQL_SERVER_INSTANCE\"],\n",
    "    \"database\": os.environ[\"MS_SQL_SERVER_DB\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for mssqlserver only\n",
    "conn_str = \"mssql+pyodbc://{user}:{password}@{host}/{database}?\" \\\n",
    "    \"driver=ODBC+Driver+17+for+SQL+Server&TrustServerCertificate=yes\".format(**credentials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conn_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = create_engine(conn_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note if you get a `pyodbc.OperationalError: SQL Server does not exist or access denied. (17) (SQLDriverConnect); [08001] ODBC SQL Server Driver][` error it means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = db.connect()\n",
    "conn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_sql(\n",
    "#     # name of sql table to be created in database\n",
    "#     name='chronicdisease', \n",
    "\n",
    "#     # \n",
    "#     con=conn, \n",
    "#     if_exists='replace', \n",
    "\n",
    "#     # Write DataFrame index as a column. Uses 'index_label' as the column name in the table.\n",
    "#     # but since we already created an ID column there is no need\n",
    "#     index=False,\n",
    "\n",
    "#     # if a pendingrollbackerror is raised it is most likely due to \n",
    "#     # chunk_size parameter in to_sql(). The table may have of columns\n",
    "#     # or rows, so the size of the packets being sent may be above the\n",
    "#     # set threshold for the mysql database if no chunksize is set\n",
    "#     chunksize=1000\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnxn = pyodbc.connect(**credentials, driver=\"{SQL Server}\")\n",
    "cnxn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor = cnxn.cursor()\n",
    "cursor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### what is the `yearstart`, `location`, `topic`, `datavalue`, `latitude`, `longitude` of an instance with the most recent year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "    SELECT TOP 3 id, yearstart, locationdesc, topic, datavalue, latitude, longitude FROM chronicdisease \n",
    "    WHERE yearstart = (\n",
    "        SELECT TOP 1 MAX(yearstart) FROM chronicdisease\n",
    "        GROUP BY yearstart\n",
    "    );\n",
    "\"\"\"\n",
    "cursor.execute(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = cursor.fetchall()\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame.from_records(data, columns=[\"id\", \"yearstart\", \"locationdesc\", \"topic\", \"datavalue\", \"latitude\", \"longitude\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alter imported table such that ID column is the primary key\n",
    "by default a column will be nullable in sql transact not unlike mysql or psql, so we have to explicitly alter the table to be not null, and then set it as a primary key since we know primary keys cannot be null and must be unique (not unlike foreign keys which can be null or duplicated).\n",
    "\n",
    "query to alter a tables column and to make it not null is `ALTER TABLE [<schema>].[<database>] ALTER COLUMN <column 1 name>, <column 2 name>, ..., <column n name> <data type e.g. BIGINT> NOT NULL;`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''\n",
    "    ALTER TABLE chronicdisease ALTER COLUMN id BIGINT NOT NULL;\n",
    "'''\n",
    "cursor.execute(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''\n",
    "    ALTER TABLE chronicdisease ADD PRIMARY KEY (id);\n",
    "'''\n",
    "cursor.execute(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From here do any query you want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''\n",
    "    SELECT TOP 3 * FROM chronicdisease; \n",
    "'''\n",
    "cursor.execute(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = cursor.fetchall()\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''\n",
    "    SELECT DISTINCT(question) FROM chronicdisease; \n",
    "'''\n",
    "cursor.execute(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = cursor.fetchall()\n",
    "data[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''\n",
    "    SELECT DISTINCT(datavalueunit) FROM chronicdisease; \n",
    "'''\n",
    "cursor.execute(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = cursor.fetchall()\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''\n",
    "    SELECT DISTINCT(datavaluetype) FROM chronicdisease;\n",
    "'''\n",
    "cursor.execute(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = cursor.fetchall()\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''\n",
    "    SELECT COUNT(datavalue) AS unique_data_value, datavalue FROM chronicdisease\n",
    "    GROUP BY datavalue; \n",
    "'''\n",
    "cursor.execute(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = cursor.fetchall()\n",
    "data[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ok so we can say if a row has a `datavalueunit` of `per 100,000 residents` and a `datavalue` of `36.7` we and the `question` is `\"Heavy drinking among adults aged >= 18 years\"`, it entails **heavy drinking among adult aged >= 18 years had a prevalence of 36.7 per 100,000 residents**\n",
    "\n",
    "But I need some way to translate these into numbers somehow. And this is where using our population table which we scraped comes into play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# populations_per_state_01_21_df.to_sql(\n",
    "#     # name of sql table to be created in database\n",
    "#     name='populationsperstate_01_21', \n",
    "\n",
    "#     # \n",
    "#     con=conn, \n",
    "#     if_exists='replace', \n",
    "\n",
    "#     # Write DataFrame index as a column. Uses 'index_label' as the column name in the table.\n",
    "#     # but since we already created an ID column there is no need    \n",
    "#     index=False,\n",
    "\n",
    "#     # if a pendingrollbackerror is raised it is most likely due to \n",
    "#     # chunk_size parameter in to_sql(). The table may have of columns\n",
    "#     # or rows, so the size of the packets being sent may be above the\n",
    "#     # set threshold for the mysql database if no chunksize is set\n",
    "#     chunksize=1000   \n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''\n",
    "    SELECT TOP 10 * FROM populationsperstate_01_21;\n",
    "'''\n",
    "cursor.execute(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = cursor.fetchall()\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now we join the two tables based on the `chronicdisease` table's `yearstart`, `yearend`, and `locationdesc`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''\n",
    "    WITH a (population, state, pyear, yearstart, yearend, locaitondesc, locationabbr, datavalue, datavalueunit, datavaluetype, question) AS (\n",
    "        SELECT population, state, year AS pyear, yearstart, yearend, locationdesc, locationabbr, datavalue, datavalueunit, datavaluetype, question\n",
    "        FROM chronicdisease\n",
    "        LEFT JOIN populationsperstate_01_21\n",
    "        ON chronicdisease.locationdesc = populationsperstate_01_21.state AND chronicdisease.yearstart = populationsperstate_01_21.year\n",
    "        WHERE state IS NOT NULL\n",
    "    )\n",
    "\n",
    "    SELECT TOP 10 * FROM a\n",
    "    WHERE pyear = 2021\n",
    "    ORDER BY population DESC; \n",
    "'''\n",
    "cursor.execute(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = cursor.fetchall()\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# writing populations_per_state_age_00_09, populations_per_state_age_10_19, populations_per_state_age_20_23 dataframes to sql server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfs = [\n",
    "#     (\"populationsperstateage_00_09\", populations_per_state_age_00_09_df), \n",
    "#     (\"populationsperstateage_10_19\", populations_per_state_age_10_19_df), \n",
    "#     (\"populationsperstateage_20_23\", populations_per_state_age_20_23_df)]\n",
    "# for table_name, df in dfs:\n",
    "#     df.to_sql(\n",
    "#         # name of sql table to be created in database\n",
    "#         name=table_name, \n",
    "\n",
    "#         # \n",
    "#         con=conn, \n",
    "#         if_exists='replace', \n",
    "\n",
    "#         # Write DataFrame index as a column. Uses 'index_label' as the column name in the table.\n",
    "#         # but since we already created an ID column there is no need    \n",
    "#         index=False,\n",
    "\n",
    "#         # if a pendingrollbackerror is raised it is most likely due to \n",
    "#         # chunk_size parameter in to_sql(). The table may have of columns\n",
    "#         # or rows, so the size of the packets being sent may be above the\n",
    "#         # set threshold for the mysql database if no chunksize is set\n",
    "#         chunksize=1000   \n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''\n",
    "    SELECT TOP 10 *\n",
    "    FROM populationsperstateage_00_09; \n",
    "'''\n",
    "cursor.execute(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = cursor.fetchall()\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''\n",
    "    SELECT TOP 10 *\n",
    "    FROM populationsperstateage_10_19; \n",
    "'''\n",
    "cursor.execute(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = cursor.fetchall()\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''\n",
    "    SELECT TOP 10 *\n",
    "    FROM populationsperstateage_20_23; \n",
    "'''\n",
    "cursor.execute(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = cursor.fetchall()\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tech-interview",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
