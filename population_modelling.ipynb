{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9d27179",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import re\n",
    "import ast\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "from utilities.preprocessors import column_summary, model_population_table, model_population_by_sex_race_ho_table, get_state_populations\n",
    "from utilities.visualizers import disp_cat_feat, view_feat_outliers\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "195640ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(51, 51, 51)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_DIR = './data/population-data'\n",
    "EXCLUSIONS = [\"us_populations_per_state_2001_to_2021.csv\"]\n",
    "files = list(filter(lambda file: not file in EXCLUSIONS, os.listdir(DATA_DIR)))\n",
    "populations_by_sex_age_00_10 = list(filter(lambda file: \"2000-2010\" in file and \"by_sex_and_age\" in file, files))\n",
    "populations_by_sex_race_ho_00_10 = list(filter(lambda file: \"2000-2010\" in file and \"by_sex_race_and_ho\" in file, files))\n",
    "populations_by_sex_age_10_19 = list(filter(lambda file: \"2010-2019\" in file and \"by_sex_and_age\" in file, files))\n",
    "populations_by_sex_race_ho_10_19 = list(filter(lambda file: \"2010-2019\" in file and \"by_sex_race_and_ho\" in file, files))\n",
    "populations_by_sex_age_20_23 = list(filter(lambda file: \"2020-2023\" in file and \"by_sex_and_age\" in file, files))\n",
    "populations_by_sex_race_ho_20_23 = list(filter(lambda file: \"2020-2023\" in file and \"by_sex_race_and_ho\" in file, files))\n",
    "len(populations_by_sex_age_00_10), len(populations_by_sex_age_10_19), len(populations_by_sex_age_20_23),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd196e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(populations_by_sex_race_ho_00_10), len(populations_by_sex_race_ho_10_19), len(populations_by_sex_race_ho_20_23),"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbbaef29",
   "metadata": {},
   "source": [
    "# Read sample excel sheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612033ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_excel(os.path.join(DATA_DIR, \"Alabama_pop_by_sex_and_age_2000-2010.xls\"), dtype=object, header=None)\n",
    "test_df.head(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675b9ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "male_start = test_df[test_df[0] == \"MALE\"].index.to_list()[0]\n",
    "male_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61bfd3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_brackets = test_df.iloc[male_start:]\n",
    "pop_brackets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d8212b",
   "metadata": {},
   "outputs": [],
   "source": [
    "female_start = pop_brackets[pop_brackets[0] == \"FEMALE\"].index.to_list()[0]\n",
    "male_end, female_end = pop_brackets[pop_brackets[0] == \".Median age (years)\"].index.to_list()\n",
    "male_end, female_end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5455187c",
   "metadata": {},
   "source": [
    "# split the excel spreadsheet into the male and female population brackets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db4397f",
   "metadata": {},
   "outputs": [],
   "source": [
    "male_pop_bracket = test_df.iloc[male_start:male_end]\n",
    "male_pop_bracket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196cb7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "female_pop_bracket = test_df.iloc[female_start:female_end]\n",
    "female_pop_bracket"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c4601b",
   "metadata": {},
   "source": [
    "#### Remove the following\n",
    "* column `1`, column `12`, and column `13` (the reasoning is these contain only the population estimates of april 1 and not the most recent one which is supposed to be at july 1, and that column `13` is the year 2010 which already exists in the next population years)\n",
    "* rows with mostly Nan and the a dot symbol in column `1` i.e. `[. Nan Nan Nan Nan Nan ... Nan]`\n",
    "* and the male column \n",
    "\n",
    "#### we also rename the columns to be `bracket`, `2000`, `2001`, `2002`, `2003`, `2004`, `2005`, `2006`, `2007`, `2008`, `2009`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb19208",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_remove = [1, 12, 13]\n",
    "cond = (male_pop_bracket[0] != \".\") & (male_pop_bracket[0] != \"MALE\")\n",
    "name_map = {0: \"bracket\", 2: 2000, 3: 2001, 4: 2002, 5: 2003, 6: 2004, 7: 2005, 8: 2006, 9: 2007, 10: 2008, 11: 2009}\n",
    "temp_male = male_pop_bracket[cond].drop(columns=cols_to_remove).rename(columns=name_map).reset_index(drop=True)\n",
    "temp_male"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce0b074",
   "metadata": {},
   "source": [
    "#### we remove the brackets that have duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe6fb19",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_male = temp_male.drop_duplicates(ignore_index=True)\n",
    "temp_male"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440bcdfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_male.index = temp_male[\"bracket\"]\n",
    "temp_male"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0adebc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "del temp_male[\"bracket\"]\n",
    "temp_male"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da3684c",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_male.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd23224",
   "metadata": {},
   "source": [
    "#### in order to achieve the ff:\n",
    "![modelling table from population data by sex and age 2000 to 2009.png](./figures%20&%20images/modelling%20table%20from%20population%20data%20by%20sex%20and%20age%202000%20to%202009.png)\n",
    "#### we need to somehow at least make our age brackets our index so that when each row is stacked vertically and the column becomes now the row index, that we are able to still keep track of our original row indeces which are our age brackets so that when the dataframe is stacked later and it becomes a multi index dataframe we can just reset the index so that our multi index of our age brackets and years now become columns themselves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0853e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_male = temp_male.stack().reset_index()\n",
    "temp_male"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0163e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_male = temp_male.rename(columns={\"level_1\": \"year\", 0: \"population\"})\n",
    "temp_male"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dcd25c0",
   "metadata": {},
   "source": [
    "#### we also apply transformations to the `bracket` column by splitting say `.5 to 9 years` to 5 and 9 and have separate columns named `age_start` and `age_end` to take in these values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab86938",
   "metadata": {},
   "outputs": [],
   "source": [
    "def helper(bracket: str | None):\n",
    "    bracket = bracket.lower()\n",
    "    keyword = re.search(r\"(under|to|and over|\\+)\", bracket)\n",
    "    keyword = np.nan if not keyword else keyword[0]\n",
    "    numbers = re.findall(r\"\\d+\", bracket)\n",
    "    numbers = [ast.literal_eval(number) for number in numbers]\n",
    "    # print(keyword)\n",
    "    # print(numbers)\n",
    "\n",
    "    # e.g. \"under 5\" becomes \"_under_5\"\n",
    "    if keyword == \"under\":\n",
    "        return (0, numbers[-1])\n",
    "    \n",
    "    # e.g. \"5 to 9\" becomes \"_5_to_9\"\n",
    "    elif keyword == \"to\":\n",
    "        return (numbers[0], numbers[-1])\n",
    "    \n",
    "    # e.g. \"9 and over\" becomes \"_9_and_over\"\n",
    "    elif keyword == \"and over\" or keyword == \"+\": \n",
    "        return (numbers[-1], float('inf'))\n",
    "    \n",
    "    # if it is a single number just return that number\n",
    "    return (np.nan, numbers[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d719222",
   "metadata": {},
   "outputs": [],
   "source": [
    "age_ranges = temp_male[\"bracket\"].apply(helper).to_list()\n",
    "age_ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b90cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_male[\"age_start\"], temp_male[\"age_end\"] = list(zip(*age_ranges))\n",
    "temp_male"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a7dc1b",
   "metadata": {},
   "source": [
    "#### delete the bracket column for the last time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb58f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "del temp_male[\"bracket\"]\n",
    "temp_male"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9d4545",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_male[\"sex\"] = \"Male\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b40cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_male[\"state\"] = \"Alabama\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf08bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_male_pop_bracket = temp_male\n",
    "final_male_pop_bracket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff61c2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "(final_male_pop_bracket[\"population\"] <= 0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6b5a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_summary(final_male_pop_bracket)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29b80b2",
   "metadata": {},
   "source": [
    "#### We've done our preprocessing on the male population age brackets now we have to this same preprocessing on the female demographic. We can achieve this by writing a function that implements our above prototype that not only does it to the male population but also that of the female one, adn combines the resulting dataframes into one single dataframe for easy collation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a2cfc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_population_table(test_df, \"Alabama\", cols_to_remove, year_range=\"2000-2009\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917e2150",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concur_model_pop_tables(file, cols_to_remove, year_range, callback_fn=model_population_table):\n",
    "    FILE_PATH = os.path.join(DATA_DIR, file)\n",
    "    state = re.search(r\"(^[A-Za-z\\s]+)\", file)\n",
    "    state = \"Unknown\" if not state else state[0]\n",
    "\n",
    "    # print(cols_to_remove)\n",
    "    # print(year_range)\n",
    "    # read excel file\n",
    "    df = pd.read_excel(FILE_PATH, dtype=object, header=None)\n",
    "    \n",
    "    state_population = callback_fn(df, state, cols_to_remove, year_range=year_range)\n",
    "    return state_population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7ed97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with ThreadPoolExecutor() as exe:\n",
    "    state_populations_by_sex_age_00_09 = list(exe.map(\n",
    "        concur_model_pop_tables, \n",
    "        populations_by_sex_age_00_10, \n",
    "        [cols_to_remove] * len(populations_by_sex_age_00_10),\n",
    "        [\"2000-2009\"] * len(populations_by_sex_age_00_10)\n",
    "    ))\n",
    "\n",
    "state_populations_by_sex_age_df_00_09 = pd.concat(state_populations_by_sex_age_00_09, axis=0, ignore_index=True)\n",
    "state_populations_by_sex_age_df_00_09[\"id\"] = state_populations_by_sex_age_df_00_09.index + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6deb1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_populations_by_sex_age_df_00_09"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f62f16",
   "metadata": {},
   "source": [
    "#### we don't save this modelled dataset as we will instead be uploading automatically using an orchestration tool like airflow to a data warehouse like databricks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0542882e",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_summary(state_populations_by_sex_age_df_00_09)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cdacb41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# take note this is just hte below five age bracket, \n",
    "# if we include all other age brackets we might have\n",
    "# a bigger total population value per year\n",
    "test = state_populations_by_sex_age_df_00_09.groupby(by=[\"year\", \"bracket\", \"sex\", \"state\"]).agg(total_population=(\"population\", \"sum\"))\n",
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa51bd8",
   "metadata": {},
   "source": [
    "# Reading sample excel file for year 2010-2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ed87b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_10_19 = pd.read_excel(os.path.join(DATA_DIR, \"Alabama_pop_by_sex_and_age_2010-2019.xlsx\"), dtype=object, header=None)\n",
    "test_df_10_19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fdbce61",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_index = test_df_10_19[test_df_10_19[0] == \".0\"].index.to_list()[0]\n",
    "start_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d474bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "end_index = test_df_10_19[test_df_10_19[0] == \".Median Age (years)\"].index.to_list()[0]\n",
    "end_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff38334c",
   "metadata": {},
   "source": [
    "#### Extract necessary rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5b7934",
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_brackets_10_19 = test_df_10_19.iloc[start_index: end_index]\n",
    "pop_brackets_10_19"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42abdbc9",
   "metadata": {},
   "source": [
    "#### remove duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295f5336",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = pop_brackets_10_19.drop_duplicates()\n",
    "temp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31cebb08",
   "metadata": {},
   "source": [
    "#### remove rows with at least 5 nan values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d0357a",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = temp.dropna(thresh=5, axis=0)\n",
    "temp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0bee502",
   "metadata": {},
   "source": [
    "#### remove columns 1 to 7, then increment by 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa03b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_remove = [1, 2, 3, 4, 5, 6] + list(range(7, temp.shape[1], 3))\n",
    "cols_to_remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ace0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = temp.drop(columns=cols_to_remove)\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558fa620",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp.index = temp[0]\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb65915",
   "metadata": {},
   "outputs": [],
   "source": [
    "del temp[0]\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f992cb89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate and create multi index for columns\n",
    "years = sorted(list(range(2010, 2020)) * 2)\n",
    "genders = [\"male\", \"female\"] * 10\n",
    "multi_index_list = list(zip(years, genders))\n",
    "multi_index_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb87dac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_index = pd.MultiIndex.from_tuples(multi_index_list)\n",
    "multi_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a7230a",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_index[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d189fbb6",
   "metadata": {},
   "source": [
    "#### set multi indexed columns and delete index name of rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9d9256",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp.columns = multi_index\n",
    "temp.index.name = \"bracket\"\n",
    "temp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f9693e",
   "metadata": {},
   "source": [
    "#### now we will have to stack each row vertically on each other and because we have multi indexed columns we will need to stack it twice in order to make these column indeces now be the row indeces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9168843",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = temp.stack().stack()\n",
    "temp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fed3442",
   "metadata": {},
   "source": [
    "#### now we can reset the index such that these multi index rows now become columns of our new dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1305f720",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = temp.reset_index()\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0e24b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename the newly converted columns to bracket, sex, year, and population respectively\n",
    "temp = temp.rename(columns={\"level_1\": \"sex\", \"level_2\": \"year\", 0: \"population\"})\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b040699",
   "metadata": {},
   "outputs": [],
   "source": [
    "age_ranges_00_10 = temp[\"bracket\"].apply(helper).to_list()\n",
    "age_ranges_00_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b239738",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp[\"age_start\"], temp[\"age_end\"] = list(zip(*age_ranges_00_10))\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8865393a",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp[\"state\"] = \"Alabama\"\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd7dcaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_population_table(test_df_10_19, \"Alabama\", cols_to_remove, year_range=\"2010-2019\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab160cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with ThreadPoolExecutor() as exe:\n",
    "    state_populations_by_sex_age_10_19 = list(exe.map(\n",
    "        concur_model_pop_tables, \n",
    "        populations_by_sex_age_10_19, \n",
    "        [cols_to_remove] * len(populations_by_sex_age_10_19),\n",
    "        [\"2010-2019\"] * len(populations_by_sex_age_10_19)\n",
    "    ))\n",
    "\n",
    "state_populations_by_sex_age_df_10_19 = pd.concat(state_populations_by_sex_age_10_19, axis=0, ignore_index=True)\n",
    "state_populations_by_sex_age_df_10_19[\"id\"] = state_populations_by_sex_age_df_10_19.index + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd68106e",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_populations_by_sex_age_df_10_19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b433218b",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_summary(state_populations_by_sex_age_df_10_19)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8324ac5",
   "metadata": {},
   "source": [
    "#### again we don't save this modelled dataset as we will instead be uploading automatically using an orchestration tool like airflow to a data warehouse like databricks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa40a38",
   "metadata": {},
   "source": [
    "# reading sample excel file from year 2020-2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86c51eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_20_23 = pd.read_excel(os.path.join(DATA_DIR, \"Alabama_pop_by_sex_and_age_2020-2023.xlsx\"), dtype=object, header=None)\n",
    "test_df_20_23"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b48337",
   "metadata": {},
   "source": [
    "#### clearly we now know we can discard columns 1, 2, 3, 4, 7, 10, and 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42bd7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_remove = [1, 2, 3, 4] + list(range(7, test_df_20_23.shape[1], 3))\n",
    "cols_to_remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf63140",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_population_table(test_df_20_23, \"Alabama\", cols_to_remove, year_range=\"2020-2023\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0bd1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "with ThreadPoolExecutor() as exe:\n",
    "    state_populations_by_sex_age_20_23 = list(exe.map(\n",
    "        concur_model_pop_tables, \n",
    "        populations_by_sex_age_20_23, \n",
    "        [cols_to_remove] * len(populations_by_sex_age_20_23),\n",
    "        [\"2020-2023\"] * len(populations_by_sex_age_20_23)\n",
    "    ))\n",
    "\n",
    "state_populations_by_sex_age_df_20_23 = pd.concat(state_populations_by_sex_age_20_23, axis=0, ignore_index=True)\n",
    "state_populations_by_sex_age_df_20_23[\"id\"] = state_populations_by_sex_age_df_20_23.index + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae3bd81",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_populations_by_sex_age_df_20_23"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77517a7e",
   "metadata": {},
   "source": [
    "#### we don't save this modelled dataset as we will instead be uploading automatically using an orchestration tool like airflow to a data warehouse like databricks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bdd3abc",
   "metadata": {},
   "source": [
    "# Modelling excel spreadsheets with population values based on sex, race, and hispanic origin 2000 - 2009"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4091d4",
   "metadata": {},
   "source": [
    "![modelling table from population data by sex race and ethnicity 2000 to 2009.png](./figures%20&%20images/modelling%20table%20from%20population%20data%20by%20sex%20race%20and%20ethnicity%202000%20to%202009.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b535e06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_00_10 = pd.read_excel(os.path.join(DATA_DIR, \"Alabama_pop_by_sex_race_and_ho_2000-2010.xls\"), dtype=object, header=None)\n",
    "test_df_00_10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c4b999",
   "metadata": {},
   "source": [
    "#### delete columns 1 and 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51687c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_remove = [1, 12, 13]\n",
    "temp = test_df_00_10.drop(columns=cols_to_remove)\n",
    "temp = temp.rename(columns={0: \"ethnicity\", 2: 2000, 3: 2001, 4: 2002, 5: 2003, 6: 2004, 7: 2005, 8: 2006, 9: 2007, 10: 2008, 11: 2009, 13: 2010})\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6205d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp[\"ethnicity\"] = temp[\"ethnicity\"].apply(lambda string: np.nan if pd.isna(string) else string.strip(\".\").lower())\n",
    "temp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32032aeb",
   "metadata": {},
   "source": [
    "#### start partitioning the spreadsheet by its important rows like the sex, and whether or not it is of hispanic origin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d9b5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "male_start = temp.index[temp[\"ethnicity\"] == \"male\"].to_list()[0]\n",
    "male_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff38987d",
   "metadata": {},
   "outputs": [],
   "source": [
    "female_start = temp.index[temp[\"ethnicity\"] == \"female\"].to_list()[0]\n",
    "female_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb88312",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp.iloc[75:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9679de19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# since there are multiple indeces with the two \n",
    "# or more races value we need to pick out the last value\n",
    "female_end = temp.index[temp[\"ethnicity\"] == \"two or more races\"].to_list()[-1]\n",
    "female_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226192df",
   "metadata": {},
   "outputs": [],
   "source": [
    "male_pop_bracket = temp.iloc[male_start:female_start].reset_index(drop=True)\n",
    "male_pop_bracket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11632d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "male_non_hisp_start = male_pop_bracket.index[male_pop_bracket[\"ethnicity\"] == \"not hispanic\"].to_list()[-1]\n",
    "male_non_hisp_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38703b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "male_hisp_start = male_pop_bracket.index[male_pop_bracket[\"ethnicity\"] == \"hispanic\"].to_list()[-1]\n",
    "male_hisp_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882b0dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "male_hisp_end = male_pop_bracket.index[male_pop_bracket[\"ethnicity\"] == \"two or more races\"].to_list()[-1]\n",
    "male_hisp_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0cdf6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "male_non_hisp_pop_bracket = male_pop_bracket.iloc[male_non_hisp_start + 2:male_hisp_start].reset_index(drop=True)\n",
    "male_non_hisp_pop_bracket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0beffa0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "male_non_hisp_pop_bracket[\"origin\"] = \"non-hispanic\"\n",
    "male_non_hisp_pop_bracket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47dc38d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "male_non_hisp_pop_bracket[\"sex\"] = \"male\"\n",
    "male_non_hisp_pop_bracket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c873edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "male_non_hisp_pop_bracket = male_non_hisp_pop_bracket.set_index(keys=[\"ethnicity\", \"origin\", \"sex\"])\n",
    "male_non_hisp_pop_bracket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836aca85",
   "metadata": {},
   "outputs": [],
   "source": [
    "male_non_hisp_pop_bracket = male_non_hisp_pop_bracket.stack().reset_index()\n",
    "male_non_hisp_pop_bracket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6674a220",
   "metadata": {},
   "outputs": [],
   "source": [
    "male_non_hisp_pop_bracket = male_non_hisp_pop_bracket.rename(columns={\"level_3\": \"year\", 0: \"population\"})\n",
    "male_non_hisp_pop_bracket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6354964",
   "metadata": {},
   "outputs": [],
   "source": [
    "male_non_hisp_pop_bracket[\"population\"] = male_non_hisp_pop_bracket[\"population\"].astype(int)\n",
    "male_non_hisp_pop_bracket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e542286b",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_summary(male_non_hisp_pop_bracket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40aa0fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "male_hisp_pop_bracket = male_pop_bracket.iloc[male_hisp_start + 2:].reset_index(drop=True)\n",
    "male_hisp_pop_bracket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f3282e",
   "metadata": {},
   "outputs": [],
   "source": [
    "male_hisp_pop_bracket[\"origin\"] = \"hispanic\"\n",
    "male_hisp_pop_bracket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803b472f",
   "metadata": {},
   "outputs": [],
   "source": [
    "male_hisp_pop_bracket[\"sex\"] = \"male\"\n",
    "male_hisp_pop_bracket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c83d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "male_hisp_pop_bracket = male_hisp_pop_bracket.set_index(keys=[\"ethnicity\", \"origin\", \"sex\"])\n",
    "male_hisp_pop_bracket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9faacbee",
   "metadata": {},
   "outputs": [],
   "source": [
    "male_hisp_pop_bracket = male_hisp_pop_bracket.stack().reset_index()\n",
    "male_hisp_pop_bracket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c3e5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "male_hisp_pop_bracket = male_hisp_pop_bracket.rename(columns={\"level_3\": \"year\", 0: \"population\"})\n",
    "male_hisp_pop_bracket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b4b502",
   "metadata": {},
   "outputs": [],
   "source": [
    "male_hisp_pop_bracket[\"population\"] = male_hisp_pop_bracket[\"population\"].astype(int)\n",
    "male_hisp_pop_bracket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a976df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_summary(male_hisp_pop_bracket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a87014",
   "metadata": {},
   "outputs": [],
   "source": [
    "female_pop_bracket = temp.iloc[female_start:female_end + 1].reset_index(drop=True)\n",
    "female_pop_bracket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175bb86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "female_non_hisp_start = female_pop_bracket.index[female_pop_bracket[\"ethnicity\"] == \"not hispanic\"].to_list()[-1]\n",
    "female_non_hisp_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce87e827",
   "metadata": {},
   "outputs": [],
   "source": [
    "female_hisp_start = female_pop_bracket.index[female_pop_bracket[\"ethnicity\"] == \"hispanic\"].to_list()[-1]\n",
    "female_hisp_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0679e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "female_non_hisp_pop_bracket = female_pop_bracket.iloc[female_non_hisp_start + 2:female_hisp_start].reset_index(drop=True)\n",
    "female_non_hisp_pop_bracket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2ca0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "female_non_hisp_pop_bracket[\"origin\"] = \"non-hispanic\"\n",
    "female_non_hisp_pop_bracket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f365b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "female_non_hisp_pop_bracket[\"sex\"] = \"female\"\n",
    "female_non_hisp_pop_bracket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d50ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "female_non_hisp_pop_bracket = female_non_hisp_pop_bracket.set_index(keys=[\"ethnicity\", \"origin\", \"sex\"])\n",
    "female_non_hisp_pop_bracket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe9e4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "female_non_hisp_pop_bracket = female_non_hisp_pop_bracket.stack().reset_index()\n",
    "female_non_hisp_pop_bracket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56e925e",
   "metadata": {},
   "outputs": [],
   "source": [
    "female_non_hisp_pop_bracket = female_non_hisp_pop_bracket.rename(columns={\"level_3\": \"year\", 0: \"population\"})\n",
    "female_non_hisp_pop_bracket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e005771",
   "metadata": {},
   "outputs": [],
   "source": [
    "female_non_hisp_pop_bracket[\"population\"] = female_non_hisp_pop_bracket[\"population\"].astype(int)\n",
    "female_non_hisp_pop_bracket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ba5424",
   "metadata": {},
   "outputs": [],
   "source": [
    "female_hisp_pop_bracket = female_pop_bracket.iloc[female_hisp_start + 2:].reset_index(drop=True)\n",
    "female_hisp_pop_bracket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25b6599",
   "metadata": {},
   "outputs": [],
   "source": [
    "female_hisp_pop_bracket[\"origin\"] = \"hispanic\"\n",
    "female_hisp_pop_bracket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4a0c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "female_hisp_pop_bracket[\"sex\"] = \"female\"\n",
    "female_hisp_pop_bracket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d2e2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "female_hisp_pop_bracket = female_hisp_pop_bracket.set_index(keys=[\"ethnicity\", \"origin\", \"sex\"])\n",
    "female_hisp_pop_bracket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05d7bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "female_hisp_pop_bracket = female_hisp_pop_bracket.stack().reset_index()\n",
    "female_hisp_pop_bracket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af54a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "female_hisp_pop_bracket = female_hisp_pop_bracket.rename(columns={\"level_3\": \"year\", 0: \"population\"})\n",
    "female_hisp_pop_bracket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf20858",
   "metadata": {},
   "outputs": [],
   "source": [
    "female_hisp_pop_bracket[\"population\"] = female_hisp_pop_bracket[\"population\"].astype(int)\n",
    "female_hisp_pop_bracket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b4b478",
   "metadata": {},
   "outputs": [],
   "source": [
    "final = pd.concat([male_non_hisp_pop_bracket, male_hisp_pop_bracket, female_non_hisp_pop_bracket, female_hisp_pop_bracket], axis=0, ignore_index=True)\n",
    "final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce292a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "final = model_population_by_sex_race_ho_table(test_df_00_10, \"Alabama\", cols_to_remove, year_range=\"2000-2009\")\n",
    "final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215347b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3778b039",
   "metadata": {},
   "outputs": [],
   "source": [
    "with ThreadPoolExecutor() as exe:\n",
    "    state_populations_by_sex_race_ho_00_09 = list(exe.map(\n",
    "        concur_model_pop_tables, \n",
    "        populations_by_sex_race_ho_00_10, \n",
    "        [cols_to_remove] * len(populations_by_sex_race_ho_00_10),\n",
    "        [\"2000-2009\"] * len(populations_by_sex_race_ho_00_10),\n",
    "        [model_population_by_sex_race_ho_table] * len(populations_by_sex_race_ho_00_10)\n",
    "    ))\n",
    "\n",
    "state_populations_by_sex_race_ho_df_00_09 = pd.concat(state_populations_by_sex_race_ho_00_09, axis=0, ignore_index=True)\n",
    "state_populations_by_sex_race_ho_df_00_09[\"id\"] = state_populations_by_sex_race_ho_df_00_09.index + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47e45e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_populations_by_sex_race_ho_df_00_09"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9998433c",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_summary(final)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d786b4",
   "metadata": {},
   "source": [
    "# Modelling excel spreadsheets with population values based on sex, race, and hispanic origin 2010 - 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59815418",
   "metadata": {},
   "source": [
    "![modelling table from population data by sex race and ethnicity 2010 to 2019.png](./figures%20&%20images/modelling%20table%20from%20population%20data%20by%20sex%20race%20and%20ethnicity%202010%20to%202019.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5117adc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_10_19 = pd.read_excel(os.path.join(DATA_DIR, \"Alabama_pop_by_sex_race_and_ho_2010-2019.xlsx\"), dtype=object, header=None)\n",
    "test_df_10_19"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527e90db",
   "metadata": {},
   "source": [
    "#### remove columns 1 and 2 and rename remaining columns to ethnicity and years 2010 to 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209580ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_remove = [1, 2]\n",
    "temp = test_df_10_19.drop(columns=cols_to_remove)\n",
    "temp = temp.rename(columns={0: \"ethnicity\", 3: 2010, 4: 2011, 5: 2012, 6: 2013, 7: 2014, 8: 2015, 9: 2016, 10: 2017, 11: 2018, 12: 2019})\n",
    "temp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f704eb",
   "metadata": {},
   "source": [
    "#### we can use set theory to use dictionary comprehension and build the new names for the columns instead always hardcoding the new names of the columns based on the years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9021b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cols_to_remove = [1, 12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9c23c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "lo_year = 2010\n",
    "hi_year = 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9cafb4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "years_list = list(range(lo_year, hi_year + 1)) * 2\n",
    "years_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c46b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_cols = list(set(test_df_10_19.columns) - set(test_cols_to_remove + [0]))\n",
    "new_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811f82d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "{new_col: years_list[i] for i, new_col in enumerate(new_cols)}\n",
    "# {new_col: \"ethnicity\" if new_col == 0 else 2 for i, new_col in enumerate(new_cols)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357b1e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp[\"ethnicity\"] = temp[\"ethnicity\"].apply(lambda string: np.nan if pd.isna(string) else string.strip(\".\").lower())\n",
    "temp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efcfba70",
   "metadata": {},
   "source": [
    "#### start partitioning the spreadsheet by its important rows like the sex, and whether or not it is of hispanic origin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d235d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "male_start = temp.index[temp[\"ethnicity\"] == \"male\"].to_list()[0]\n",
    "male_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0207690c",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp.iloc[male_start]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2357dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "female_start = temp.index[temp[\"ethnicity\"] == \"female\"].to_list()[0]\n",
    "female_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e475ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# since there are multiple indeces with the two \n",
    "# or more races value we need to pick out the last value\n",
    "female_end = temp.index[temp[\"ethnicity\"] == \"two or more races\"].to_list()[-1]\n",
    "female_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe39eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp.iloc[female_start: female_end + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d802a803",
   "metadata": {},
   "outputs": [],
   "source": [
    "male_pop_bracket = temp.iloc[male_start:female_start].reset_index(drop=True)\n",
    "male_pop_bracket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74acbd9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "male_non_hisp_start = male_pop_bracket.index[male_pop_bracket[\"ethnicity\"] == \"not hispanic\"].to_list()[0]\n",
    "male_non_hisp_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d97f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the first occurence of the index as we are \n",
    "# not looking for multiple occurences until it reaches \n",
    "# the last occurence\n",
    "male_non_hisp_end = male_pop_bracket.loc[male_non_hisp_start:, :] \\\n",
    ".index[male_pop_bracket.loc[male_non_hisp_start:, \"ethnicity\"].str.contains(\"race alone or in combination\")].to_list()[0]\n",
    "male_non_hisp_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fef3063",
   "metadata": {},
   "outputs": [],
   "source": [
    "male_hisp_start = male_pop_bracket.index[male_pop_bracket[\"ethnicity\"] == \"hispanic\"].to_list()[-1]\n",
    "male_hisp_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e59cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "male_hisp_end = male_pop_bracket.index[male_pop_bracket[\"ethnicity\"] == \"two or more races\"].to_list()[-1]\n",
    "male_hisp_end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3246bfa9",
   "metadata": {},
   "source": [
    "#### once table is partioned by hispanic origin and sex we will now add the origin and sex columns and do typical stacking afterwards "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7eae7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "male_non_hisp_pop_bracket = male_pop_bracket.iloc[male_non_hisp_start + 2:male_non_hisp_end].reset_index(drop=True)\n",
    "male_non_hisp_pop_bracket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56146694",
   "metadata": {},
   "outputs": [],
   "source": [
    "male_hisp_pop_bracket = male_pop_bracket.iloc[male_hisp_start + 2:male_hisp_end + 1].reset_index(drop=True)\n",
    "male_hisp_pop_bracket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b63ac9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "male_non_hisp_pop_bracket[\"origin\"] = \"non-hispanic\"\n",
    "male_hisp_pop_bracket[\"origin\"] = \"hispanic\"\n",
    "male_non_hisp_pop_bracket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaad884d",
   "metadata": {},
   "outputs": [],
   "source": [
    "male_non_hisp_pop_bracket[\"sex\"] = \"male\"\n",
    "male_hisp_pop_bracket[\"sex\"] = \"male\"\n",
    "male_non_hisp_pop_bracket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559f2353",
   "metadata": {},
   "outputs": [],
   "source": [
    "male_non_hisp_pop_bracket = male_non_hisp_pop_bracket.set_index(keys=[\"ethnicity\", \"origin\", \"sex\"])\n",
    "male_hisp_pop_bracket = male_hisp_pop_bracket.set_index(keys=[\"ethnicity\", \"origin\", \"sex\"])\n",
    "male_hisp_pop_bracket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f7ee70",
   "metadata": {},
   "outputs": [],
   "source": [
    "male_non_hisp_pop_bracket = male_non_hisp_pop_bracket.stack().reset_index()\n",
    "male_hisp_pop_bracket = male_hisp_pop_bracket.stack().reset_index()\n",
    "male_hisp_pop_bracket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c37a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "male_non_hisp_pop_bracket = male_non_hisp_pop_bracket.rename(columns={\"level_3\": \"year\", 0: \"population\"})\n",
    "male_hisp_pop_bracket = male_hisp_pop_bracket.rename(columns={\"level_3\": \"year\", 0: \"population\"})\n",
    "male_non_hisp_pop_bracket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c5a07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "male_non_hisp_pop_bracket[\"population\"] = male_non_hisp_pop_bracket[\"population\"].astype(int)\n",
    "male_hisp_pop_bracket[\"population\"] = male_hisp_pop_bracket[\"population\"].astype(int)\n",
    "column_summary(male_hisp_pop_bracket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95106596",
   "metadata": {},
   "outputs": [],
   "source": [
    "female_pop_bracket = temp.iloc[female_start:female_end + 1].reset_index(drop=True)\n",
    "female_pop_bracket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f257b077",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the list slices here for origin\n",
    "female_non_hisp_start = female_pop_bracket.index[female_pop_bracket[\"ethnicity\"] == \"not hispanic\"].to_list()[-1]\n",
    "\n",
    "# get the first occurence of the index as we are \n",
    "# not looking for multiple occurences until it reaches \n",
    "# the last occurence\n",
    "female_non_hisp_end = female_pop_bracket.loc[female_non_hisp_start:, :] \\\n",
    ".index[female_pop_bracket.loc[female_non_hisp_start:, \"ethnicity\"].str.contains(\"race alone or in combination\")].to_list()[0]\n",
    "\n",
    "female_non_hisp_start, female_non_hisp_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afab469c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with ThreadPoolExecutor() as exe:\n",
    "    state_populations_by_sex_race_ho_10_19 = list(exe.map(\n",
    "        concur_model_pop_tables, \n",
    "        populations_by_sex_race_ho_10_19, \n",
    "        [cols_to_remove] * len(populations_by_sex_race_ho_10_19),\n",
    "        [\"2010-2019\"] * len(populations_by_sex_race_ho_10_19),\n",
    "        [model_population_by_sex_race_ho_table] * len(populations_by_sex_race_ho_10_19)\n",
    "    ))\n",
    "\n",
    "state_populations_by_sex_race_ho_df_10_19 = pd.concat(state_populations_by_sex_race_ho_10_19, axis=0, ignore_index=True)\n",
    "state_populations_by_sex_race_ho_df_10_19[\"id\"] = state_populations_by_sex_race_ho_df_10_19.index + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1edf2fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_populations_by_sex_race_ho_df_10_19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e180c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_populations_by_sex_race_ho_df_00_09[\"ethnicity\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26bd9b45",
   "metadata": {},
   "source": [
    "# Modelling population table by sex, race, hispanic origin years 2020 to 2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8d822d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_remove = [1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1010c3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with ThreadPoolExecutor() as exe:\n",
    "    state_populations_by_sex_race_ho_20_23 = list(exe.map(\n",
    "        concur_model_pop_tables, \n",
    "        populations_by_sex_race_ho_20_23, \n",
    "        [cols_to_remove] * len(populations_by_sex_race_ho_20_23),\n",
    "        [\"2020-2023\"] * len(populations_by_sex_race_ho_20_23),\n",
    "        [model_population_by_sex_race_ho_table] * len(populations_by_sex_race_ho_20_23)\n",
    "    ))\n",
    "\n",
    "state_populations_by_sex_race_ho_df_20_23 = pd.concat(state_populations_by_sex_race_ho_20_23, axis=0, ignore_index=True)\n",
    "state_populations_by_sex_race_ho_df_20_23[\"id\"] = state_populations_by_sex_race_ho_df_20_23.index + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f5f554",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_populations_by_sex_race_ho_df_20_23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d29e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_remove_00_09 = [1, 12, 13]\n",
    "cols_to_remove_10_19 = [1, 2, 3, 4, 5, 6, 7, 10, 13, 16, 19, 22, 25, 28, 31, 34]\n",
    "cols_to_remove_20_23 = [1, 2, 3, 4, 7, 10, 13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb61a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_populations_by_sex_age_df_00_09 = get_state_populations(\n",
    "    DATA_DIR, \n",
    "    cols_to_remove=cols_to_remove_00_09, \n",
    "    populations=populations_by_sex_age_00_10, \n",
    "    year_range=\"2000-2009\",\n",
    "    by=\"sex and age\")\n",
    "state_populations_by_sex_age_df_00_09"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2bae94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_populations_by_sex_age_df_10_19 = get_state_populations(\n",
    "    DATA_DIR, \n",
    "    cols_to_remove=cols_to_remove_10_19, \n",
    "    populations=populations_by_sex_age_10_19, \n",
    "    year_range=\"2010-2019\",\n",
    "    by=\"sex and age\")\n",
    "state_populations_by_sex_age_df_10_19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1412c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_populations_by_sex_age_df_20_23 = get_state_populations(\n",
    "    DATA_DIR, \n",
    "    cols_to_remove=cols_to_remove_20_23, \n",
    "    populations=populations_by_sex_age_20_23, \n",
    "    year_range=\"2020-2023\",\n",
    "    by=\"sex and age\")\n",
    "state_populations_by_sex_age_df_20_23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2303648",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_populations_by_sex_race_ho_df_00_09 = get_state_populations(\n",
    "    DATA_DIR, \n",
    "    cols_to_remove=[1, 12, 13], \n",
    "    populations=populations_by_sex_race_ho_00_10, \n",
    "    year_range=\"2000-2009\",\n",
    "    by=\"sex race and ho\")\n",
    "state_populations_by_sex_race_ho_df_00_09"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b627cf46",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_populations_by_sex_race_ho_df_10_19 = get_state_populations(\n",
    "    DATA_DIR, \n",
    "    cols_to_remove=[1, 2], \n",
    "    populations=populations_by_sex_race_ho_10_19, \n",
    "    year_range=\"2010-2019\",\n",
    "    by=\"sex race and ho\")\n",
    "state_populations_by_sex_race_ho_df_10_19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdfaefdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_populations_by_sex_race_ho_df_20_23 = get_state_populations(\n",
    "    DATA_DIR, \n",
    "    cols_to_remove=[1], \n",
    "    populations=populations_by_sex_race_ho_20_23, \n",
    "    year_range=\"2020-2023\",\n",
    "    by=\"sex race and ho\")\n",
    "state_populations_by_sex_race_ho_df_20_23"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57400ca",
   "metadata": {},
   "source": [
    "# Converting all code to pyspark for faster processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe78f385",
   "metadata": {},
   "outputs": [],
   "source": [
    "# note that pyarrow 4.0.0 is a dependency of pyspark pandas api\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql.types import StringType, StructField, StructType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9ab2877",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\LARRY\\\\anaconda3\\\\envs\\\\tech-interview\\\\Lib\\\\site-packages\\\\pyspark\\\\__init__.py'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyspark.__file__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed2202d",
   "metadata": {},
   "source": [
    "#### if pyspark is not yet added to our path upon installation in our environment or globally we will need to locate the bin directory inside pyspark directory and add the bin directory path to our `PATH` environment variable. Why we do this is so we can run spark-submit and other spark related commands in our command line."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0332b95",
   "metadata": {},
   "source": [
    "* if an error \n",
    "```\n",
    "25/04/22 12:52:59 WARN Shell: Did not find winutils.exe: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\n",
    "Python was not found; run without arguments to install from the Microsoft Store, or disable this shortcut from Settings > Apps > Advanced app settings > App execution aliases.\n",
    "25/04/22 12:52:59 INFO ShutdownHookManager: Shutdown hook called\n",
    "25/04/22 12:52:59 INFO ShutdownHookManager: Deleting directory C:\\Users\\LARRY\\AppData\\Local\\Temp\\spark-b0654aae-f91c-442d-b27a-66b287ffd557\n",
    "```\n",
    "occurs this means that we have to install winutils via pip in our conda environment or globally in our  local machine.\n",
    "\n",
    "* another solution is gooing to manage app execution aliases and turning off python and python3: https://stackoverflow.com/questions/65348890/python-was-not-found-run-without-arguments-to-install-from-the-microsoft-store\n",
    "\n",
    "* another error connected to the above is...\n",
    "```\n",
    "Missing Python executable 'python3', defaulting to 'C:\\Users\\LARRY\\anaconda3\\envs\\tech-interview\\Scripts\\..' for SPARK_HOME environment variable. Please install Python or specify the correct Python executable in PYSPARK_DRIVER_PYTHON or PYSPARK_PYTHON environment variable to detect SPARK_HOME safely.\n",
    "The system cannot find the path specified.\n",
    "The system cannot find the path specified.\n",
    "```\n",
    "this maybe due to dependency errors and certain values not being added to the path system environment variable or an environment variable not being added such as `SPARK_HOME`, `HADOOP_HOME`, and `JAVA_HOME` as system environment variables containing the installation location of these softwares\n",
    "\n",
    "take note that spark 3.5.4 requires java 8 or 17 and later. When on the downloads page it will also indicate that it is prevuilt for hadoop 3.3 and later meaning we have to install hadoop 3.3.0 and later releases (but specifically the winutils executable file as it requires winutils) and must be under these release versions.\n",
    "\n",
    "steps for setting up apache spark from scratch\n",
    "- java development kit 17: https://www.oracle.com/java/technologies/javase/jdk17-archive-downloads.html\n",
    "- apache spark: https://spark.apache.org/downloads.html\n",
    "- hadoop winutils: https://github.com/kontext-tech/winutils/blob/master/hadoop-3.3.0/bin/winutils.exe\n",
    "- once downloaded extract the `spark-3.x.x-bin.hadoop3.tgz`\n",
    "- rename the extracted folder `spark-3.x.x-bin.hadoop3` to just `spark-3.x.x`\n",
    "- once jdk17 is downloaded run executable file and install JDK and keep track fo installation location which is commonly at `C:\\Program Files\\Java\\jdk-17` \n",
    "- create folder named hadoop and inside it create sub directory/ named bin and move the downloaded hadoop `winutils.exe` file inside\n",
    "- move the spark and hadoop folders in any directory or perhaps the `C:\\Program Files` directory\n",
    "- copy the `C:\\Program Files\\spark-3.5.5`, `C:\\Program Files\\hadoop`, `C:\\Program Files\\Java\\jdk-17` paths which contain the bin files of spark, hadoop, and jdk 17\n",
    "- add new system environment variables named `SPARK_HOME`, `HADOOP_HOME`, and `JAVA_HOME`, with these values respectively. AH so now I know that you can download these software in a docker container and replicate the same process of copying their installation paths and creating system environment variables through `export JAVA_HOME=\"installation/dir/of/jdk\"`, `export SPARK_HOME=\"installation/dir/of/jdk\"`, `export HADOOP_HOME=\"installation/dir/of/hadoop\"` (however note this inly does it for the current shell and all processes in current shell if you want to do it globally or add it as a system environment variable you need to use `sudo -H gedit /etc/environment`)\n",
    "- in windows we can reference these system environment variables as `%<name of env var>%` e.g. `%SPARK_HOME%` and we'd get the value we assigned to this environment/system environment variable and add backslashes to it to reference sub directories in this directory e.g. `%SPARK_HOME%/bin` will be `C:\\Program Files\\spark-3.5.5\\bin`. In linux we use `$<name of env var>`. But we add new values to the system path environment variable where we will now reference these newly created system environment variables. We add `%SPARK_HOME%\\bin`, `%HADOOP_HOME%\\bin`, and `%JAVA_HOME%\\bin`\n",
    "- restart the command line and run `javac --version`, `spark-shell`, to check if the installed software has been installed and commands are able to run in command line. spark-shell is a CLI for spark. Now we can use `spark-submit` for our python scripts containing spark sessions\n",
    "- we need to also add PYSPARK_HOME containing the path to our global python interpreter which would be in path `C:\\Users\\LARRY\\AppData\\Local\\Programs\\Python\\Python312\\` and appended to it the `python.exe` e.g. `C:\\Users\\LARRY\\AppData\\Local\\Programs\\Python\\Python312\\python.exe` as this string will be needed in order for `spark-submit` to run our python scripts containing spark commands\n",
    "\n",
    "```\n",
    "C:\\Users\\LARRY>spark-shell\n",
    "Setting default log level to \"WARN\".\n",
    "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
    "25/04/22 13:50:20 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
    "25/04/22 13:50:22 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
    "Spark context Web UI available at http://LAPTOP-3GL266K9.bbrouter:4041\n",
    "Spark context available as 'sc' (master = local[*], app id = local-1745301022738).\n",
    "Spark session available as 'spark'.\n",
    "Welcome to\n",
    "      ____              __\n",
    "     / __/__  ___ _____/ /__\n",
    "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
    "   /___/ .__/\\_,_/_/ /_/\\_\\   version 3.5.5\n",
    "      /_/\n",
    "\n",
    "Using Scala version 2.12.18 (Java HotSpot(TM) 64-Bit Server VM, Java 17.0.12)\n",
    "Type in expressions to have them evaluated.\n",
    "Type :help for more information.\n",
    "\n",
    "scala>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1cf4228e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./data/population-data\\\\Alabama_pop_by_sex_and_age_2000-2010.xls'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = os.path.join(DATA_DIR, \"Alabama_pop_by_sex_and_age_2000-2010.xls\")\n",
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a2c2aeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>table with row headers in column A and column ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Table 2. Intercensal Estimates of the Resident...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sex and Age</td>\n",
       "      <td>April 1, 20001</td>\n",
       "      <td>Intercensal Estimates (as of July 1)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>April 1, 20102</td>\n",
       "      <td>July 1, 20103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2000</td>\n",
       "      <td>2001</td>\n",
       "      <td>2002</td>\n",
       "      <td>2003</td>\n",
       "      <td>2004</td>\n",
       "      <td>2005</td>\n",
       "      <td>2006</td>\n",
       "      <td>2007</td>\n",
       "      <td>2008</td>\n",
       "      <td>2009</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BOTH SEXES</td>\n",
       "      <td>4447207</td>\n",
       "      <td>4452173</td>\n",
       "      <td>4467634</td>\n",
       "      <td>4480089</td>\n",
       "      <td>4503491</td>\n",
       "      <td>4530729</td>\n",
       "      <td>4569805</td>\n",
       "      <td>4628981</td>\n",
       "      <td>4672840</td>\n",
       "      <td>4718206</td>\n",
       "      <td>4757938</td>\n",
       "      <td>4779736</td>\n",
       "      <td>4785298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>Note: Median age is calculated based on single...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>Suggested Citation:</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>Table 2. Intercensal Estimates of the Resident...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>Source: U.S. Census Bureau, Population Division</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>Release Date: October 2012</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>117 rows  14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    0               1   \\\n",
       "0    table with row headers in column A and column ...             NaN   \n",
       "1    Table 2. Intercensal Estimates of the Resident...             NaN   \n",
       "2                                          Sex and Age  April 1, 20001   \n",
       "3                                                  NaN             NaN   \n",
       "4                                           BOTH SEXES         4447207   \n",
       "..                                                 ...             ...   \n",
       "112  Note: Median age is calculated based on single...             NaN   \n",
       "113                                Suggested Citation:             NaN   \n",
       "114  Table 2. Intercensal Estimates of the Resident...             NaN   \n",
       "115    Source: U.S. Census Bureau, Population Division             NaN   \n",
       "116                         Release Date: October 2012             NaN   \n",
       "\n",
       "                                       2        3        4        5        6   \\\n",
       "0                                     NaN      NaN      NaN      NaN      NaN   \n",
       "1                                     NaN      NaN      NaN      NaN      NaN   \n",
       "2    Intercensal Estimates (as of July 1)      NaN      NaN      NaN      NaN   \n",
       "3                                    2000     2001     2002     2003     2004   \n",
       "4                                 4452173  4467634  4480089  4503491  4530729   \n",
       "..                                    ...      ...      ...      ...      ...   \n",
       "112                                   NaN      NaN      NaN      NaN      NaN   \n",
       "113                                   NaN      NaN      NaN      NaN      NaN   \n",
       "114                                   NaN      NaN      NaN      NaN      NaN   \n",
       "115                                   NaN      NaN      NaN      NaN      NaN   \n",
       "116                                   NaN      NaN      NaN      NaN      NaN   \n",
       "\n",
       "          7        8        9        10       11              12  \\\n",
       "0        NaN      NaN      NaN      NaN      NaN             NaN   \n",
       "1        NaN      NaN      NaN      NaN      NaN             NaN   \n",
       "2        NaN      NaN      NaN      NaN      NaN  April 1, 20102   \n",
       "3       2005     2006     2007     2008     2009             NaN   \n",
       "4    4569805  4628981  4672840  4718206  4757938         4779736   \n",
       "..       ...      ...      ...      ...      ...             ...   \n",
       "112      NaN      NaN      NaN      NaN      NaN             NaN   \n",
       "113      NaN      NaN      NaN      NaN      NaN             NaN   \n",
       "114      NaN      NaN      NaN      NaN      NaN             NaN   \n",
       "115      NaN      NaN      NaN      NaN      NaN             NaN   \n",
       "116      NaN      NaN      NaN      NaN      NaN             NaN   \n",
       "\n",
       "                13  \n",
       "0              NaN  \n",
       "1              NaN  \n",
       "2    July 1, 20103  \n",
       "3              NaN  \n",
       "4          4785298  \n",
       "..             ...  \n",
       "112            NaN  \n",
       "113            NaN  \n",
       "114            NaN  \n",
       "115            NaN  \n",
       "116            NaN  \n",
       "\n",
       "[117 rows x 14 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df_00_10 = pd.read_excel(path, dtype=object, header=None)\n",
    "test_df_00_10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc8435b",
   "metadata": {},
   "source": [
    "run command `spark-submit --packages com.crealytics:spark-excel_2.12:3.5.1_0.20.4 test_submit.py` in order to execute this spark script it is imperative to add this packages argument as this indicates the dependency that we need installed when running this script transforming excel files\n",
    "\n",
    "`com.crealytics:spark-excel_2.12:3.5.1_0.20.4` is actuall ythe package we need to read these excel files using spark where `com.crealytics` is the group id, `spark-excel_2.12` is the artifact id, and `3.5.1_0.20.4` is the release version\n",
    "\n",
    "but how come this works when using spark-submit but when using jupyter notebooks the extra packages are not downloaded "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d0a6def",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.conf.SparkConf at 0x274f9a83b30>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf = SparkConf()\n",
    "conf.set(\"spark.jars.packages\", \"com.crealytics:spark-excel_2.12:3.5.1_0.20.4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "38ca1279",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n: java.lang.RuntimeException: java.io.FileNotFoundException: Hadoop bin directory does not exist: C:\\ProgramData\\hadoop\\bin\\bin -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:735)\r\n\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:270)\r\n\tat org.apache.hadoop.fs.FileUtil.chmod(FileUtil.java:1139)\r\n\tat org.apache.hadoop.fs.FileUtil.chmod(FileUtil.java:1125)\r\n\tat org.apache.spark.util.Utils$.fetchFile(Utils.scala:489)\r\n\tat org.apache.spark.SparkContext.addFile(SparkContext.scala:1790)\r\n\tat org.apache.spark.SparkContext.$anonfun$new$16(SparkContext.scala:528)\r\n\tat org.apache.spark.SparkContext.$anonfun$new$16$adapted(SparkContext.scala:528)\r\n\tat scala.collection.immutable.List.foreach(List.scala:431)\r\n\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:528)\r\n\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\r\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)\r\n\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\r\n\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:499)\r\n\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:480)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:238)\r\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\r\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:842)\r\nCaused by: java.io.FileNotFoundException: Hadoop bin directory does not exist: C:\\ProgramData\\hadoop\\bin\\bin -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.getQualifiedBinInner(Shell.java:607)\r\n\tat org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:591)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:688)\r\n\tat org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDurationHelper(Configuration.java:1907)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1867)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1840)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)\r\n\tat org.apache.hadoop.util.ShutdownHookManager$HookEntry.<init>(ShutdownHookManager.java:207)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.addShutdownHook(ShutdownHookManager.java:304)\r\n\tat org.apache.spark.util.SparkShutdownHookManager.install(ShutdownHookManager.scala:181)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks$lzycompute(ShutdownHookManager.scala:50)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks(ShutdownHookManager.scala:48)\r\n\tat org.apache.spark.util.ShutdownHookManager$.addShutdownHook(ShutdownHookManager.scala:153)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<init>(ShutdownHookManager.scala:58)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<clinit>(ShutdownHookManager.scala)\r\n\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:242)\r\n\tat org.apache.spark.util.SparkFileUtils.createTempDir(SparkFileUtils.scala:103)\r\n\tat org.apache.spark.util.SparkFileUtils.createTempDir$(SparkFileUtils.scala:102)\r\n\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:94)\r\n\tat org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:377)\r\n\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:969)\r\n\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:199)\r\n\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:222)\r\n\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:91)\r\n\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1125)\r\n\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1134)\r\n\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPy4JJavaError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m spark = \u001b[43mSparkSession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbuilder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mappName\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtest\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\LARRY\\anaconda3\\envs\\tech-interview\\Lib\\site-packages\\pyspark\\sql\\session.py:497\u001b[39m, in \u001b[36mSparkSession.Builder.getOrCreate\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    495\u001b[39m     sparkConf.set(key, value)\n\u001b[32m    496\u001b[39m \u001b[38;5;66;03m# This SparkContext may be an existing one.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m497\u001b[39m sc = \u001b[43mSparkContext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msparkConf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[38;5;66;03m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[32m    499\u001b[39m \u001b[38;5;66;03m# by all sessions.\u001b[39;00m\n\u001b[32m    500\u001b[39m session = SparkSession(sc, options=\u001b[38;5;28mself\u001b[39m._options)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\LARRY\\anaconda3\\envs\\tech-interview\\Lib\\site-packages\\pyspark\\context.py:515\u001b[39m, in \u001b[36mSparkContext.getOrCreate\u001b[39m\u001b[34m(cls, conf)\u001b[39m\n\u001b[32m    513\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext._lock:\n\u001b[32m    514\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m SparkContext._active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m515\u001b[39m         \u001b[43mSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mSparkConf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    516\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m SparkContext._active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    517\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m SparkContext._active_spark_context\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\LARRY\\anaconda3\\envs\\tech-interview\\Lib\\site-packages\\pyspark\\context.py:203\u001b[39m, in \u001b[36mSparkContext.__init__\u001b[39m\u001b[34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[39m\n\u001b[32m    201\u001b[39m SparkContext._ensure_initialized(\u001b[38;5;28mself\u001b[39m, gateway=gateway, conf=conf)\n\u001b[32m    202\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m203\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_do_init\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    204\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmaster\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    205\u001b[39m \u001b[43m        \u001b[49m\u001b[43mappName\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    206\u001b[39m \u001b[43m        \u001b[49m\u001b[43msparkHome\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    207\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpyFiles\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    208\u001b[39m \u001b[43m        \u001b[49m\u001b[43menvironment\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    209\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbatchSize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    210\u001b[39m \u001b[43m        \u001b[49m\u001b[43mserializer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    211\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    212\u001b[39m \u001b[43m        \u001b[49m\u001b[43mjsc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    213\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprofiler_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    214\u001b[39m \u001b[43m        \u001b[49m\u001b[43mudf_profiler_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    215\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmemory_profiler_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    216\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    217\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:\n\u001b[32m    218\u001b[39m     \u001b[38;5;66;03m# If an error occurs, clean up in order to allow future SparkContext creation:\u001b[39;00m\n\u001b[32m    219\u001b[39m     \u001b[38;5;28mself\u001b[39m.stop()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\LARRY\\anaconda3\\envs\\tech-interview\\Lib\\site-packages\\pyspark\\context.py:296\u001b[39m, in \u001b[36mSparkContext._do_init\u001b[39m\u001b[34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[39m\n\u001b[32m    293\u001b[39m \u001b[38;5;28mself\u001b[39m.environment[\u001b[33m\"\u001b[39m\u001b[33mPYTHONHASHSEED\u001b[39m\u001b[33m\"\u001b[39m] = os.environ.get(\u001b[33m\"\u001b[39m\u001b[33mPYTHONHASHSEED\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m0\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    295\u001b[39m \u001b[38;5;66;03m# Create the Java SparkContext through Py4J\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m296\u001b[39m \u001b[38;5;28mself\u001b[39m._jsc = jsc \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_initialize_context\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_conf\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_jconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    297\u001b[39m \u001b[38;5;66;03m# Reset the SparkConf to the one actually used by the SparkContext in JVM.\u001b[39;00m\n\u001b[32m    298\u001b[39m \u001b[38;5;28mself\u001b[39m._conf = SparkConf(_jconf=\u001b[38;5;28mself\u001b[39m._jsc.sc().conf())\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\LARRY\\anaconda3\\envs\\tech-interview\\Lib\\site-packages\\pyspark\\context.py:421\u001b[39m, in \u001b[36mSparkContext._initialize_context\u001b[39m\u001b[34m(self, jconf)\u001b[39m\n\u001b[32m    417\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    418\u001b[39m \u001b[33;03mInitialize SparkContext in function to allow subclass specific initialization\u001b[39;00m\n\u001b[32m    419\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    420\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m._jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m421\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jvm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mJavaSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjconf\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\LARRY\\anaconda3\\envs\\tech-interview\\Lib\\site-packages\\py4j\\java_gateway.py:1587\u001b[39m, in \u001b[36mJavaClass.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1581\u001b[39m command = proto.CONSTRUCTOR_COMMAND_NAME +\\\n\u001b[32m   1582\u001b[39m     \u001b[38;5;28mself\u001b[39m._command_header +\\\n\u001b[32m   1583\u001b[39m     args_command +\\\n\u001b[32m   1584\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1586\u001b[39m answer = \u001b[38;5;28mself\u001b[39m._gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1587\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1588\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_gateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fqn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1590\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1591\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\LARRY\\anaconda3\\envs\\tech-interview\\Lib\\site-packages\\py4j\\protocol.py:326\u001b[39m, in \u001b[36mget_return_value\u001b[39m\u001b[34m(answer, gateway_client, target_id, name)\u001b[39m\n\u001b[32m    324\u001b[39m value = OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[32m2\u001b[39m:], gateway_client)\n\u001b[32m    325\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[32m1\u001b[39m] == REFERENCE_TYPE:\n\u001b[32m--> \u001b[39m\u001b[32m326\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[32m    327\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    328\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name), value)\n\u001b[32m    329\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    330\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[32m    331\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    332\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name, value))\n",
      "\u001b[31mPy4JJavaError\u001b[39m: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n: java.lang.RuntimeException: java.io.FileNotFoundException: Hadoop bin directory does not exist: C:\\ProgramData\\hadoop\\bin\\bin -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:735)\r\n\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:270)\r\n\tat org.apache.hadoop.fs.FileUtil.chmod(FileUtil.java:1139)\r\n\tat org.apache.hadoop.fs.FileUtil.chmod(FileUtil.java:1125)\r\n\tat org.apache.spark.util.Utils$.fetchFile(Utils.scala:489)\r\n\tat org.apache.spark.SparkContext.addFile(SparkContext.scala:1790)\r\n\tat org.apache.spark.SparkContext.$anonfun$new$16(SparkContext.scala:528)\r\n\tat org.apache.spark.SparkContext.$anonfun$new$16$adapted(SparkContext.scala:528)\r\n\tat scala.collection.immutable.List.foreach(List.scala:431)\r\n\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:528)\r\n\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\r\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)\r\n\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\r\n\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:499)\r\n\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:480)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:238)\r\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\r\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:842)\r\nCaused by: java.io.FileNotFoundException: Hadoop bin directory does not exist: C:\\ProgramData\\hadoop\\bin\\bin -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.getQualifiedBinInner(Shell.java:607)\r\n\tat org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:591)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:688)\r\n\tat org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDurationHelper(Configuration.java:1907)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1867)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1840)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)\r\n\tat org.apache.hadoop.util.ShutdownHookManager$HookEntry.<init>(ShutdownHookManager.java:207)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.addShutdownHook(ShutdownHookManager.java:304)\r\n\tat org.apache.spark.util.SparkShutdownHookManager.install(ShutdownHookManager.scala:181)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks$lzycompute(ShutdownHookManager.scala:50)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks(ShutdownHookManager.scala:48)\r\n\tat org.apache.spark.util.ShutdownHookManager$.addShutdownHook(ShutdownHookManager.scala:153)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<init>(ShutdownHookManager.scala:58)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<clinit>(ShutdownHookManager.scala)\r\n\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:242)\r\n\tat org.apache.spark.util.SparkFileUtils.createTempDir(SparkFileUtils.scala:103)\r\n\tat org.apache.spark.util.SparkFileUtils.createTempDir$(SparkFileUtils.scala:102)\r\n\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:94)\r\n\tat org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:377)\r\n\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:969)\r\n\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:199)\r\n\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:222)\r\n\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:91)\r\n\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1125)\r\n\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1134)\r\n\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\r\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName('test')\\\n",
    "    .config(conf=conf)\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a73bfe3",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o238.load.\n: org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: com.crealytics.spark.excel. Please find packages at `https://spark.apache.org/third-party-projects.html`.\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:725)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)\r\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:208)\r\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:186)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:842)\r\nCaused by: java.lang.ClassNotFoundException: com.crealytics.spark.excel.DefaultSource\r\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\r\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:592)\r\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:633)\r\n\tat scala.util.Try$.apply(Try.scala:213)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:633)\r\n\tat scala.util.Failure.orElse(Try.scala:224)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:633)\r\n\t... 15 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPy4JJavaError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[62]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m test_spark_df_00_10 = \u001b[43mspark\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m.\u001b[49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcom.crealytics.spark.excel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mheader\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtrue\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minferSchema\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtrue\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\LARRY\\anaconda3\\envs\\tech-interview\\Lib\\site-packages\\pyspark\\sql\\readwriter.py:307\u001b[39m, in \u001b[36mDataFrameReader.load\u001b[39m\u001b[34m(self, path, format, schema, **options)\u001b[39m\n\u001b[32m    305\u001b[39m \u001b[38;5;28mself\u001b[39m.options(**options)\n\u001b[32m    306\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m307\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jreader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    308\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    309\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(path) != \u001b[38;5;28mlist\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\LARRY\\anaconda3\\envs\\tech-interview\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1316\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1317\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1318\u001b[39m     args_command +\\\n\u001b[32m   1319\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1321\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1323\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1325\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1326\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\LARRY\\anaconda3\\envs\\tech-interview\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdeco\u001b[39m(*a: Any, **kw: Any) -> Any:\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m179\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    180\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    181\u001b[39m         converted = convert_exception(e.java_exception)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\LARRY\\anaconda3\\envs\\tech-interview\\Lib\\site-packages\\py4j\\protocol.py:326\u001b[39m, in \u001b[36mget_return_value\u001b[39m\u001b[34m(answer, gateway_client, target_id, name)\u001b[39m\n\u001b[32m    324\u001b[39m value = OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[32m2\u001b[39m:], gateway_client)\n\u001b[32m    325\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[32m1\u001b[39m] == REFERENCE_TYPE:\n\u001b[32m--> \u001b[39m\u001b[32m326\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[32m    327\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    328\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name), value)\n\u001b[32m    329\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    330\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[32m    331\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    332\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name, value))\n",
      "\u001b[31mPy4JJavaError\u001b[39m: An error occurred while calling o238.load.\n: org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: com.crealytics.spark.excel. Please find packages at `https://spark.apache.org/third-party-projects.html`.\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:725)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)\r\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:208)\r\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:186)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:842)\r\nCaused by: java.lang.ClassNotFoundException: com.crealytics.spark.excel.DefaultSource\r\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\r\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:592)\r\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:633)\r\n\tat scala.util.Try$.apply(Try.scala:213)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:633)\r\n\tat scala.util.Failure.orElse(Try.scala:224)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:633)\r\n\t... 15 more\r\n"
     ]
    }
   ],
   "source": [
    "test_spark_df_00_10 = spark.read.format(\"com.crealytics.spark.excel\")\\\n",
    "    .option(\"header\", \"false\")\\\n",
    "    .option(\"inferSchema\", \"true\")\\\n",
    "    .load(path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tech-interview",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
